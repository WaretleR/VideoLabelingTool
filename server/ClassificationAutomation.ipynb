{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "unusual-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from facebookTimesformer.TimeSformer.models.vit import TimeSformer\n",
    "from facebookTimesformer.TimeSformer.datasets.utils import tensor_normalize\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sklearn\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "disciplinary-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "viratPath = os.path.join('static', 'VIRAT')\n",
    "testVideoName = 'VIRAT_S_000102.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "every-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataMean = [0.45, 0.45, 0.45]\n",
    "dataStd = [0.225, 0.225, 0.225]\n",
    "\n",
    "embeddingsSize = 768\n",
    "secondsPerAction = 7\n",
    "framesPerVideo = 8\n",
    "batchSize = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bibliographic-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventInfo():\n",
    "    def __init__(self, eventType, duration, startFrame, endFrame, framesDict):\n",
    "        self.eventType = int(eventType)\n",
    "        self.duration = int(duration)\n",
    "        self.startFrame = int(startFrame)\n",
    "        self.endFrame = int(endFrame)\n",
    "        self.framesDict = framesDict\n",
    "        self.leftTopX = 10000\n",
    "        self.leftTopY = 10000\n",
    "        self.rightBottomX = 0\n",
    "        self.rightBottomY = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "raised-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabels(labelNames):\n",
    "    eventLabelDict = {}\n",
    "    for labelName in labelNames:\n",
    "        videoName = labelName.split('.')[0]\n",
    "            \n",
    "        f = open(os.path.join(viratPath, 'labels', labelName))\n",
    "        label = f.read().split('\\n')[:-1]\n",
    "        f.close()\n",
    "        label = [l.split(' ')[:-1] for l in label]\n",
    "        label = [[int(l1) for l1 in l] for l in label]\n",
    "                \n",
    "        for l in label:\n",
    "            eventName = videoName + '.' + str(l[0])\n",
    "            if eventName not in eventLabelDict:\n",
    "                eventLabelDict[eventName] = EventInfo(l[1], l[2], l[3], l[4], dict())\n",
    "            eventLabelDict[eventName].framesDict[l[5]] = [e for e in l[6:]]\n",
    "            \n",
    "    return eventLabelDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fundamental-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels = getLabels([v for v in sorted(os.listdir(os.path.join(viratPath, 'labels'))) if not v.startswith(testVideoName)])\n",
    "testLabels = getLabels([testVideoName])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "brown-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findClosest(indexList, i):\n",
    "    #print('Closest to ' + str(i) + ' is ' + str(min(indexList, key=lambda x:abs(x-i))))\n",
    "    return min(indexList, key=lambda x:abs(x-i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "recreational-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "idMax = max([int(t.split('.')[-1]) for t in trainLabels])\n",
    "if idMax < max([int(t.split('.')[-1]) for t in testLabels]):\n",
    "    idMax = max([int(t.split('.')[-1]) for t in testLabels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "female-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCroppedClips(labels, splitClips=False):\n",
    "\n",
    "    clips = []\n",
    "    clipLabels = []\n",
    "\n",
    "    for videoName in sorted(os.listdir(os.path.join(viratPath, 'videos'))):\n",
    "        video = cv2.VideoCapture(os.path.join(viratPath, 'videos', videoName))\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        success, image = video.read()\n",
    "        actionLength = video.get(cv2.CAP_PROP_FPS) * secondsPerAction\n",
    "        \n",
    "        if splitClips:\n",
    "            events = {}\n",
    "            idNewAmount = 0\n",
    "            for key, val in labels.items():\n",
    "                if key.split('.')[0] == videoName.split('.')[0]:\n",
    "                    currentStart = val.startFrame\n",
    "                    while currentStart < val.endFrame:\n",
    "                        idNewAmount += 1\n",
    "                        print('id %d has length %d, startFrame %d, endFrame %d' % (idMax + idNewAmount, min(val.endFrame - currentStart, actionLength), currentStart, min(currentStart + actionLength, val.endFrame)))\n",
    "                        events[idMax + idNewAmount] = EventInfo(val.eventType, \n",
    "                                                                min(val.endFrame - currentStart, actionLength),\n",
    "                                                                currentStart,\n",
    "                                                                min(currentStart + actionLength, val.endFrame),\n",
    "                                                                {k:v for k, v in val.framesDict.items() if k >= currentStart and k <= currentStart + actionLength})\n",
    "                        \n",
    "                        if len(events[idMax + idNewAmount].framesDict) == 0:\n",
    "                            if val.endFrame - currentStart <= currentStart - val.startFrame:\n",
    "                                events[idMax + idNewAmount].framesDict[currentStart] = val.framesDict[val.endFrame]\n",
    "                                events[idMax + idNewAmount].framesDict[currentStart + actionLength] = val.framesDict[val.endFrame]\n",
    "                            else:\n",
    "                                events[idMax + idNewAmount].framesDict[currentStart] = val.framesDict[val.startFrame]\n",
    "                                if val.endFrame - currentStart - actionLength <= currentStart - val.startFrame + actionLength:\n",
    "                                    events[idMax + idNewAmount].framesDict[currentStart + actionLength] = val.framesDict[val.endFrame]\n",
    "                                else:\n",
    "                                    events[idMax + idNewAmount].framesDict[currentStart + actionLength] = val.framesDict[val.startFrame]\n",
    "                                \n",
    "                        currentStart += actionLength\n",
    "        else:\n",
    "            events = {key:val for key, val in labels.items() if key.split('.')[0] == videoName.split('.')[0]} \n",
    "        \n",
    "        for k in events:\n",
    "            print(\"Processing eventID %s\" % k)\n",
    "            clip = torch.zeros((framesPerVideo, 224, 224, 3), dtype = torch.uint8)\n",
    "            frameNumbers = [round(events[k].startFrame + i * (events[k].duration - 1.0) / (framesPerVideo - 1)) for i in range(framesPerVideo)]\n",
    "            frameNumbers = [findClosest(events[k].framesDict.keys(), i) for i in frameNumbers]\n",
    "            #print(frameNumbers)\n",
    "\n",
    "            for i, n in enumerate(frameNumbers):\n",
    "                xmin, ymin, xlen, ylen = events[k].framesDict[n]  \n",
    "                #print(xmin, ymin, xlen, ylen)\n",
    "                \n",
    "                if xmin < events[k].leftTopX:\n",
    "                    events[k].leftTopX = xmin\n",
    "                if ymin < events[k].leftTopY:\n",
    "                    events[k].leftTopY = ymin\n",
    "                \n",
    "                if xmin + xlen > events[k].rightBottomX:\n",
    "                    events[k].rightBottomX = xmin + xlen\n",
    "                if ymin + ylen > events[k].rightBottomY:\n",
    "                    events[k].rightBottomY = ymin + ylen\n",
    "            \n",
    "            #print(\"(\" + str(events[k].leftTopX) + \", \" + str(events[k].leftTopY) + \") (\" + str(events[k].rightBottomX) + \", \" + str(events[k].rightBottomY) + \")\")\n",
    "            #print(str(events[k].duration) + \" frames\")\n",
    "            \n",
    "            for i, n in enumerate(frameNumbers):\n",
    "                \n",
    "                video.set(cv2.CAP_PROP_POS_FRAMES, n)\n",
    "                success, image = video.read()\n",
    "                if success:\n",
    "                    bbox = image[max(events[k].leftTopY, 0) : events[k].rightBottomY, max(events[k].leftTopX, 0) : events[k].rightBottomX]\n",
    "                    \n",
    "                    clip[i] = torch.from_numpy(cv2.resize(bbox, (224, 224)))\n",
    "\n",
    "                    '''if i == len(frameNumbers) - 1:\n",
    "                        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "                        \n",
    "                        clipMedian = np.zeros((3, 224, 224), dtype = np.int32)\n",
    "                        for f in clip:\n",
    "                            clipMedian += f\n",
    "                        clipMedian = clipMedian / len(frameNumbers)\n",
    "                        clipMedian = np.swapaxes(np.swapaxes(clipMedian, 0, 2), 0, 1)\n",
    "                        \n",
    "                        ax.imshow(clipMedian.astype(int))\n",
    "                        #rect = mpatches.Rectangle((labelInfo[k].leftTopX, labelInfo[k].leftTopY), \n",
    "                        #              labelInfo[k].rightBottomX - labelInfo[k].leftTopX, labelInfo[k].rightBottomY - labelInfo[k].leftTopY,\n",
    "                        #              fill=False, edgecolor='red', linewidth=2)\n",
    "                        #ax.add_patch(rect)\n",
    "                        ax.set_axis_off()\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()'''\n",
    "\n",
    "                else:\n",
    "                    print('Error while reading frame %d in video %s' % (n, videoName))\n",
    "\n",
    "            clips.append(clip)\n",
    "            clipLabels.append(events[k].eventType)\n",
    "            \n",
    "    return (clips, clipLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "democratic-steering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing eventID VIRAT_S_010000_01_000184_000324.2\n",
      "Processing eventID VIRAT_S_010000_01_000184_000324.3\n",
      "Processing eventID VIRAT_S_010201_07_000601_000697.4\n",
      "Processing eventID VIRAT_S_010202_06_000784_000873.5\n",
      "Processing eventID VIRAT_S_010202_06_000784_000873.6\n",
      "Processing eventID VIRAT_S_010203_06_000620_000760.7\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY = getCroppedClips(trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "resident-niagara",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 8 has length 209, startFrame 0, endFrame 209\n",
      "id 9 has length 209, startFrame 209, endFrame 419\n",
      "id 10 has length 209, startFrame 419, endFrame 629\n",
      "id 11 has length 209, startFrame 629, endFrame 839\n",
      "id 12 has length 209, startFrame 839, endFrame 1048\n",
      "id 13 has length 209, startFrame 1048, endFrame 1258\n",
      "id 14 has length 209, startFrame 1258, endFrame 1468\n",
      "id 15 has length 209, startFrame 1468, endFrame 1678\n",
      "id 16 has length 209, startFrame 1678, endFrame 1888\n",
      "id 17 has length 209, startFrame 1888, endFrame 2097\n",
      "id 18 has length 209, startFrame 2097, endFrame 2307\n",
      "id 19 has length 209, startFrame 2307, endFrame 2517\n",
      "id 20 has length 209, startFrame 2517, endFrame 2727\n",
      "id 21 has length 209, startFrame 2727, endFrame 2937\n",
      "id 22 has length 209, startFrame 2937, endFrame 3146\n",
      "id 23 has length 209, startFrame 3146, endFrame 3356\n",
      "id 24 has length 209, startFrame 3356, endFrame 3566\n",
      "id 25 has length 209, startFrame 3566, endFrame 3776\n",
      "id 26 has length 209, startFrame 3776, endFrame 3986\n",
      "id 27 has length 209, startFrame 3986, endFrame 4195\n",
      "id 28 has length 209, startFrame 4195, endFrame 4405\n",
      "id 29 has length 209, startFrame 4405, endFrame 4615\n",
      "id 30 has length 209, startFrame 4615, endFrame 4825\n",
      "id 31 has length 209, startFrame 4825, endFrame 5034\n",
      "id 32 has length 209, startFrame 5034, endFrame 5244\n",
      "id 33 has length 209, startFrame 5244, endFrame 5454\n",
      "id 34 has length 209, startFrame 5454, endFrame 5664\n",
      "id 35 has length 209, startFrame 5664, endFrame 5874\n",
      "id 36 has length 209, startFrame 5874, endFrame 6083\n",
      "id 37 has length 209, startFrame 6083, endFrame 6293\n",
      "id 38 has length 209, startFrame 6293, endFrame 6503\n",
      "id 39 has length 209, startFrame 6503, endFrame 6713\n",
      "id 40 has length 209, startFrame 6713, endFrame 6923\n",
      "id 41 has length 209, startFrame 6923, endFrame 7132\n",
      "id 42 has length 209, startFrame 7132, endFrame 7342\n",
      "id 43 has length 209, startFrame 7342, endFrame 7552\n",
      "id 44 has length 209, startFrame 7552, endFrame 7762\n",
      "id 45 has length 209, startFrame 7762, endFrame 7972\n",
      "id 46 has length 209, startFrame 7972, endFrame 8181\n",
      "id 47 has length 209, startFrame 8181, endFrame 8391\n",
      "id 48 has length 209, startFrame 8391, endFrame 8601\n",
      "id 49 has length 209, startFrame 8601, endFrame 8811\n",
      "id 50 has length 209, startFrame 8811, endFrame 9020\n",
      "id 51 has length 209, startFrame 9020, endFrame 9230\n",
      "id 52 has length 209, startFrame 9230, endFrame 9440\n",
      "id 53 has length 209, startFrame 9440, endFrame 9650\n",
      "id 54 has length 209, startFrame 9650, endFrame 9860\n",
      "id 55 has length 209, startFrame 9860, endFrame 10069\n",
      "id 56 has length 209, startFrame 10069, endFrame 10279\n",
      "id 57 has length 209, startFrame 10279, endFrame 10489\n",
      "id 58 has length 209, startFrame 10489, endFrame 10699\n",
      "id 59 has length 209, startFrame 10699, endFrame 10909\n",
      "id 60 has length 209, startFrame 10909, endFrame 11118\n",
      "id 61 has length 209, startFrame 11118, endFrame 11328\n",
      "id 62 has length 209, startFrame 11328, endFrame 11538\n",
      "id 63 has length 209, startFrame 11538, endFrame 11748\n",
      "id 64 has length 209, startFrame 11748, endFrame 11958\n",
      "id 65 has length 209, startFrame 11958, endFrame 12167\n",
      "id 66 has length 209, startFrame 12167, endFrame 12377\n",
      "id 67 has length 209, startFrame 12377, endFrame 12587\n",
      "id 68 has length 209, startFrame 12587, endFrame 12797\n",
      "id 69 has length 209, startFrame 12797, endFrame 13006\n",
      "id 70 has length 209, startFrame 13006, endFrame 13216\n",
      "id 71 has length 209, startFrame 13216, endFrame 13426\n",
      "id 72 has length 209, startFrame 13426, endFrame 13636\n",
      "id 73 has length 209, startFrame 13636, endFrame 13846\n",
      "id 74 has length 209, startFrame 13846, endFrame 14055\n",
      "id 75 has length 209, startFrame 14055, endFrame 14265\n",
      "id 76 has length 209, startFrame 14265, endFrame 14475\n",
      "id 77 has length 209, startFrame 14475, endFrame 14685\n",
      "id 78 has length 209, startFrame 14685, endFrame 14895\n",
      "id 79 has length 209, startFrame 14895, endFrame 15104\n",
      "id 80 has length 209, startFrame 15104, endFrame 15314\n",
      "id 81 has length 209, startFrame 15314, endFrame 15524\n",
      "id 82 has length 209, startFrame 15524, endFrame 15734\n",
      "id 83 has length 209, startFrame 15734, endFrame 15944\n",
      "id 84 has length 209, startFrame 15944, endFrame 16153\n",
      "id 85 has length 209, startFrame 16153, endFrame 16363\n",
      "id 86 has length 209, startFrame 16363, endFrame 16573\n",
      "id 87 has length 209, startFrame 16573, endFrame 16783\n",
      "id 88 has length 209, startFrame 16783, endFrame 16993\n",
      "id 89 has length 209, startFrame 16993, endFrame 17202\n",
      "id 90 has length 209, startFrame 17202, endFrame 17412\n",
      "id 91 has length 209, startFrame 17412, endFrame 17622\n",
      "id 92 has length 209, startFrame 17622, endFrame 17832\n",
      "id 93 has length 209, startFrame 17832, endFrame 18041\n",
      "id 94 has length 209, startFrame 18041, endFrame 18251\n",
      "id 95 has length 209, startFrame 18251, endFrame 18461\n",
      "id 96 has length 209, startFrame 18461, endFrame 18671\n",
      "id 97 has length 209, startFrame 18671, endFrame 18881\n",
      "id 98 has length 209, startFrame 18881, endFrame 19090\n",
      "id 99 has length 209, startFrame 19090, endFrame 19300\n",
      "id 100 has length 209, startFrame 19300, endFrame 19510\n",
      "id 101 has length 209, startFrame 19510, endFrame 19720\n",
      "id 102 has length 209, startFrame 19720, endFrame 19930\n",
      "id 103 has length 209, startFrame 19930, endFrame 20139\n",
      "id 104 has length 209, startFrame 20139, endFrame 20349\n",
      "id 105 has length 209, startFrame 20349, endFrame 20559\n",
      "id 106 has length 209, startFrame 20559, endFrame 20769\n",
      "id 107 has length 209, startFrame 20769, endFrame 20979\n",
      "id 108 has length 209, startFrame 20979, endFrame 21188\n",
      "id 109 has length 209, startFrame 21188, endFrame 21398\n",
      "id 110 has length 209, startFrame 21398, endFrame 21608\n",
      "id 111 has length 209, startFrame 21608, endFrame 21818\n",
      "id 112 has length 209, startFrame 21818, endFrame 22027\n",
      "id 113 has length 172, startFrame 22027, endFrame 22200\n",
      "Processing eventID 8\n",
      "Processing eventID 9\n",
      "Processing eventID 10\n",
      "Processing eventID 11\n",
      "Processing eventID 12\n",
      "Processing eventID 13\n",
      "Processing eventID 14\n",
      "Processing eventID 15\n",
      "Processing eventID 16\n",
      "Processing eventID 17\n",
      "Processing eventID 18\n",
      "Processing eventID 19\n",
      "Processing eventID 20\n",
      "Processing eventID 21\n",
      "Processing eventID 22\n",
      "Processing eventID 23\n",
      "Processing eventID 24\n",
      "Processing eventID 25\n",
      "Processing eventID 26\n",
      "Processing eventID 27\n",
      "Processing eventID 28\n",
      "Processing eventID 29\n",
      "Processing eventID 30\n",
      "Processing eventID 31\n",
      "Processing eventID 32\n",
      "Processing eventID 33\n",
      "Processing eventID 34\n",
      "Processing eventID 35\n",
      "Processing eventID 36\n",
      "Processing eventID 37\n",
      "Processing eventID 38\n",
      "Processing eventID 39\n",
      "Processing eventID 40\n",
      "Processing eventID 41\n",
      "Processing eventID 42\n",
      "Processing eventID 43\n",
      "Processing eventID 44\n",
      "Processing eventID 45\n",
      "Processing eventID 46\n",
      "Processing eventID 47\n",
      "Processing eventID 48\n",
      "Processing eventID 49\n",
      "Processing eventID 50\n",
      "Processing eventID 51\n",
      "Processing eventID 52\n",
      "Processing eventID 53\n",
      "Processing eventID 54\n",
      "Processing eventID 55\n",
      "Processing eventID 56\n",
      "Processing eventID 57\n",
      "Processing eventID 58\n",
      "Processing eventID 59\n",
      "Processing eventID 60\n",
      "Processing eventID 61\n",
      "Processing eventID 62\n",
      "Processing eventID 63\n",
      "Processing eventID 64\n",
      "Processing eventID 65\n",
      "Processing eventID 66\n",
      "Processing eventID 67\n",
      "Processing eventID 68\n",
      "Processing eventID 69\n",
      "Processing eventID 70\n",
      "Processing eventID 71\n",
      "Processing eventID 72\n",
      "Processing eventID 73\n",
      "Processing eventID 74\n",
      "Processing eventID 75\n",
      "Processing eventID 76\n",
      "Processing eventID 77\n",
      "Processing eventID 78\n",
      "Processing eventID 79\n",
      "Processing eventID 80\n",
      "Processing eventID 81\n",
      "Processing eventID 82\n",
      "Processing eventID 83\n",
      "Processing eventID 84\n",
      "Processing eventID 85\n",
      "Processing eventID 86\n",
      "Processing eventID 87\n",
      "Processing eventID 88\n",
      "Processing eventID 89\n",
      "Processing eventID 90\n",
      "Processing eventID 91\n",
      "Processing eventID 92\n",
      "Processing eventID 93\n",
      "Processing eventID 94\n",
      "Processing eventID 95\n",
      "Processing eventID 96\n",
      "Processing eventID 97\n",
      "Processing eventID 98\n",
      "Processing eventID 99\n",
      "Processing eventID 100\n",
      "Processing eventID 101\n",
      "Processing eventID 102\n",
      "Processing eventID 103\n",
      "Processing eventID 104\n",
      "Processing eventID 105\n",
      "Processing eventID 106\n",
      "Processing eventID 107\n",
      "Processing eventID 108\n",
      "Processing eventID 109\n",
      "Processing eventID 110\n",
      "Processing eventID 111\n",
      "Processing eventID 112\n",
      "Processing eventID 113\n"
     ]
    }
   ],
   "source": [
    "testX, _ = getCroppedClips(testLabels, splitClips=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "waiting-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimeSformer(img_size=224, num_frames=framesPerVideo, attention_type='divided_space_time',  pretrained_model='./facebookTimesformer/weights/TimeSformer_divST_8x32_224_K600.pyth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "incoming-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "model.model.head = Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "mounted-roberts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TimeSformer(\n",
       "  (model): VisionTransformer(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (time_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print(torch.cuda.device_count())\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "alpine-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainEmb = np.zeros((len(trainX), embeddingsSize), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "extreme-study",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexander\\.conda\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "trainTensor = torch.zeros((len(trainX), 3, framesPerVideo, 224, 224))\n",
    "for i in range(len(trainX)):\n",
    "    trainTensor[i] = np.transpose(tensor_normalize(torch.tensor(trainX[i]), dataMean, dataStd), axes=[3, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "brave-router",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 6\n",
      "1 of 6\n",
      "2 of 6\n",
      "3 of 6\n",
      "4 of 6\n",
      "5 of 6\n"
     ]
    }
   ],
   "source": [
    "indices = [i for i in range(len(trainX))]\n",
    "\n",
    "for batchNumber in range(len(trainX) // batchSize):\n",
    "    print(str(batchNumber) + \" of \" + str(len(trainX) // batchSize))\n",
    "    #print(indices[batchNumber * batchSize : (batchNumber + 1) * batchSize])\n",
    "    inputs = trainTensor[indices[batchNumber * batchSize : (batchNumber + 1) * batchSize]]\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    outputs = outputs.cpu().detach().numpy()\n",
    "    trainEmb[indices[batchNumber * batchSize : (batchNumber + 1) * batchSize]] = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "military-january",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "common-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "testEmb = np.zeros((len(testX), embeddingsSize), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "announced-proposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexander\\.conda\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "testTensor = torch.zeros((len(testX), 3, framesPerVideo, 224, 224))\n",
    "for i in range(len(testX)):\n",
    "    testTensor[i] = np.transpose(tensor_normalize(torch.tensor(testX[i]), dataMean, dataStd), axes=[3, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "magnetic-raising",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 106\n",
      "1 of 106\n",
      "2 of 106\n",
      "3 of 106\n",
      "4 of 106\n",
      "5 of 106\n",
      "6 of 106\n",
      "7 of 106\n",
      "8 of 106\n",
      "9 of 106\n",
      "10 of 106\n",
      "11 of 106\n",
      "12 of 106\n",
      "13 of 106\n",
      "14 of 106\n",
      "15 of 106\n",
      "16 of 106\n",
      "17 of 106\n",
      "18 of 106\n",
      "19 of 106\n",
      "20 of 106\n",
      "21 of 106\n",
      "22 of 106\n",
      "23 of 106\n",
      "24 of 106\n",
      "25 of 106\n",
      "26 of 106\n",
      "27 of 106\n",
      "28 of 106\n",
      "29 of 106\n",
      "30 of 106\n",
      "31 of 106\n",
      "32 of 106\n",
      "33 of 106\n",
      "34 of 106\n",
      "35 of 106\n",
      "36 of 106\n",
      "37 of 106\n",
      "38 of 106\n",
      "39 of 106\n",
      "40 of 106\n",
      "41 of 106\n",
      "42 of 106\n",
      "43 of 106\n",
      "44 of 106\n",
      "45 of 106\n",
      "46 of 106\n",
      "47 of 106\n",
      "48 of 106\n",
      "49 of 106\n",
      "50 of 106\n",
      "51 of 106\n",
      "52 of 106\n",
      "53 of 106\n",
      "54 of 106\n",
      "55 of 106\n",
      "56 of 106\n",
      "57 of 106\n",
      "58 of 106\n",
      "59 of 106\n",
      "60 of 106\n",
      "61 of 106\n",
      "62 of 106\n",
      "63 of 106\n",
      "64 of 106\n",
      "65 of 106\n",
      "66 of 106\n",
      "67 of 106\n",
      "68 of 106\n",
      "69 of 106\n",
      "70 of 106\n",
      "71 of 106\n",
      "72 of 106\n",
      "73 of 106\n",
      "74 of 106\n",
      "75 of 106\n",
      "76 of 106\n",
      "77 of 106\n",
      "78 of 106\n",
      "79 of 106\n",
      "80 of 106\n",
      "81 of 106\n",
      "82 of 106\n",
      "83 of 106\n",
      "84 of 106\n",
      "85 of 106\n",
      "86 of 106\n",
      "87 of 106\n",
      "88 of 106\n",
      "89 of 106\n",
      "90 of 106\n",
      "91 of 106\n",
      "92 of 106\n",
      "93 of 106\n",
      "94 of 106\n",
      "95 of 106\n",
      "96 of 106\n",
      "97 of 106\n",
      "98 of 106\n",
      "99 of 106\n",
      "100 of 106\n",
      "101 of 106\n",
      "102 of 106\n",
      "103 of 106\n",
      "104 of 106\n",
      "105 of 106\n"
     ]
    }
   ],
   "source": [
    "indices = [i for i in range(len(testX))]\n",
    "\n",
    "for batchNumber in range(len(testX) // batchSize):\n",
    "    print(str(batchNumber) + \" of \" + str(len(testX) // batchSize))\n",
    "    #print(indices[batchNumber * batchSize : (batchNumber + 1) * batchSize])\n",
    "    inputs = testTensor[indices[batchNumber * batchSize : (batchNumber + 1) * batchSize]]\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    outputs = outputs.cpu().detach().numpy()\n",
    "    testEmb[indices[batchNumber * batchSize : (batchNumber + 1) * batchSize]] = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "rising-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('VIRAT_S_000006.pickle', 'wb') as handle:\n",
    "    pickle.dump(testEmb, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "large-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainImg = [t[0].permute(2, 0, 1) / 256. for t in trainX]\n",
    "testImg = [t[0].permute(2, 0, 1) / 256. for t in testX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "divine-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "writer.add_embedding(np.concatenate((trainEmb, testEmb)),\n",
    "                metadata=trainY + ['undefined' for i in range(len(testEmb))],\n",
    "                label_img=torch.stack(trainImg + testImg)\n",
    "                )\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "interpreted-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''logReg = LogisticRegression(C = 0.001)\n",
    "svm = SVC(C = 0.005, kernel='linear', probability = True)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "rf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "ada = AdaBoostClassifier()\n",
    "clf = make_pipeline(StandardScaler(), logReg)'''\n",
    "\n",
    "def getClf():\n",
    "    return make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "short-decrease",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('kneighborsclassifier', KNeighborsClassifier(n_neighbors=1))])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = getClf()\n",
    "    \n",
    "clf.fit(torch.tensor(trainEmb), torch.tensor(trainY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "split-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(testEmb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "addressed-tomorrow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "public-telephone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x218417a5e88>]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plt.grid()\n",
    "plt.plot([i * 30 * secondsPerAction for i in range(len(preds))], preds[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-equity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
